---
title: "Amazon Web Services (AWS) Batch"
titleshort: "AWS Batch"
description: |
  Create batch task and run batch tasks using ECR image, and save results to S3.
date: 2020-10-17
date_start: 2020-10-17
output:
  pdf_document:
    pandoc_args: '../../../_output_kniti_pdf.yaml'
    includes:
      in_header: '../../../preamble.tex'
  html_document:
    pandoc_args: '../../../_output_kniti_html.yaml'
    includes:
      in_header: "../../../hdga.html"
always_allow_html: true
urlcolor: blue
---

### AWS Batch Run

```{r global_options, include = FALSE}
try(source("../../../.Rprofile"))
```

`r text_shared_preamble_one`
`r text_shared_preamble_two`
`r text_shared_preamble_thr`

#### Create A Batch Job Definition

Before running batch jobs across EC2 Instances, need to create jobs first. These jobs need to specify:

1. computing requirements: memory and cpu: *vCpus = 1* and *Memory=7168* for example
2. which container to pull from (ECR): List the image name: *XXXX7367XXXX.dkr.ecr.us-east-1.amazonaws.com/fanconda* for example
3. job role ARN: *arn:aws:iam::710673677961:role/ecsExecutionRole* to allow for proper in and out from and to the container.

These can be registered programmatically by using boto3: [Boto3 Batch Documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/batch.html)

In the example below, will register a new job definition:

```{python}
import boto3
import yaml
import pprint

# Load YAML file
son_aws_yml = "C:/Users/fan/fanwangecon.github.io/_data/aws.yml"
fl_yaml = open(son_aws_yml)
ls_dict_yml = yaml.load(fl_yaml, Loader=yaml.BaseLoader)
# Get the first element of the yml list of dicts
aws_yml_dict_yml = ls_dict_yml[0]

# Use AWS Personal Access Keys etc to start boto3 client
aws_batch = boto3.client('batch',
  aws_access_key_id=aws_yml_dict_yml['aws_access_key_id'],
  aws_secret_access_key=aws_yml_dict_yml['aws_secret_access_key'],
  region_name=aws_yml_dict_yml['region'])

```
